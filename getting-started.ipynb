{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b7bb00-e7bb-4b89-91e9-f1e9a5e7067b",
   "metadata": {},
   "source": [
    "# Analisando dados com PySpark\n",
    "\n",
    "Neste notebook, utilizaremos o PySpark para exemplificar como:\n",
    "\n",
    "- Extrair dados de um arquivo\n",
    "- Transformar os dados para extrair as informações relevantes\n",
    "- Analisar as estatísticas de interesse\n",
    "\n",
    "Para isso, utilizaremos o [MovieLens](https://grouplens.org/datasets/movielens/), um banco de dados com avaliações de usuários para filmes cujos arquivos necessários para este notebook se encontram na pasta `./data/input`.\n",
    "\n",
    "Com base no dataset, queremos responder a seguinte pergunta: \n",
    "\n",
    "> **Quantos filmes foram lançados em cada ano do dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b09133-7ccb-4acb-829f-6dd18f117694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2333b0d1-b595-46ae-9872-e88554eefcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865c14d-1e2b-4137-bfcf-9ae694d06c91",
   "metadata": {},
   "source": [
    "# Spark\n",
    "\n",
    "## Inicializando uma SparkSession\n",
    "\n",
    "A `SparkSession` encapsula uma série de funcionalidades e contextos para utilizarmos nas aplicações do Spark, como:\n",
    "\n",
    "- SparkContext\n",
    "- SQLContext\n",
    "- HiveContext\n",
    "- Streaming Application\n",
    "\n",
    "Para criar uma `SparkSession`, utilizaremos os seguintes métodos:\n",
    "1. **Builder()** para definir as configurações da sessão\n",
    "2. **appName(**_nome_**)** para definir um nome da sessão\n",
    "3. **getOrCreate()** para criar uma sessão com tais configurações (caso uma sessão com as mesmas configurações já exista, ela será retornada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdb701f-39ae-4bee-b152-cb87c0e3c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e5852-603b-4cae-ac49-087bb5bb7ff5",
   "metadata": {},
   "source": [
    "## Extraindo os dados\n",
    "\n",
    "Os dados de entrada se encontram na pasta `input`. Note que os dados estão em múltiplos arquivos csv. Para isso, utilizaremos os seguintes métodos do módulo `SparkSession.read` para criar um dataframe:\n",
    "1. **format(**_formato_**)**: para definir o formato do arquivo \n",
    "2. **option(**_chave_, _valor_**)**: para definir possíveis configurações de leitura\n",
    "3. **load(**_caminho_do_arquivo_**)**: para carregar o arquivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4609a2-4dde-445c-94e9-128be89adafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3e9ca9-6654-46e3-b79d-735082b06b60",
   "metadata": {},
   "source": [
    "## Transformando os dados\n",
    "\n",
    "Agora que temos um dataframe em mãos, podemos fazer transformações em suas colunas. Em particular, utilizaremos os seguintes métodos:\n",
    "\n",
    "1. **withColumnRenamed(**_nome_antigo_, _novo_nome_**)** para renomear a coluna `movieId` para `movie_id`\n",
    "2. **withColumn(**_nova_coluna_, _funcao_**)** para aplicar uma função em uma coluna\n",
    "3. **pyspark.sql.functions.regexp_extract(**_coluna_, _regexp_**)** para extrair o ano dofilme da coluna `title` (utilize a expressão regular `\\((\\d{4})\\)$`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f83e46-b5fa-4964-aded-a68e79f8d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2653fa-928c-4383-a146-6f92ea7f9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd98cb-42f7-480c-9c4f-dfd9673ba368",
   "metadata": {},
   "source": [
    "## Analisando os dados\n",
    "\n",
    "Uma das funcionalidades que facilita a análise de dados utilizando o Spark é o `SparkSQL`, em que podemos fazer consultas nos dados como se estivéssmos utilizando um banco de dados relacional.\n",
    "\n",
    "A utilização do `SparkSQL` pode ser feita através de 2 métodos:\n",
    "\n",
    "1. **DataFrame.createOrReplaceTempView(**_nome_**)** para criarmos uma view consultável a partir do dataframe\n",
    "2. **SessionSpark.sql(**_query_**)** para realizar a consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc141e-b3f3-4e6e-965e-46596c56be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crie uma view temporaria a partir do dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16bd9db-0ce1-44ef-b6c7-39f692021f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_string = ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
